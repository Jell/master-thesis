%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{On the Client Side}
\label{cha:on_the_client_side}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The iPhone}
\label{sec:the_iphone}

\subsection{The device}
The iPhone is a last generation smartphone developed by Apple and was revealed for the first time in 2007. It features advanced functionalities such as multimedia support, internet access, video games and many more. Up to this day, forty-two millions iPhone have been sold across the globe.\\

Its main characteristic is that its User Interface (UI) is almost only based on two inputs:
\begin{itemize}
\item{a large multi-touch screen}
\item{a 3-axis accelerometers}
\end{itemize}
As such, the iPhone does not posses any physical keyboard. Instead, a virtual keyboard can be summoned any time it is necessary, leaving a larger screen for applications.\\

The last generation of iPhone, the iPhone 3G-S, also includes a GPS and a compass, and an upcoming version of its Operating System (OS) will allow the access to the video data of the camera.\\

In order to install new applications on an iPhone, one must download it from the AppStore, the internet-based retail store from Apple for iPhones, iPods and the iPad. Apple having a highly proprietary policy on its product, this is actually the only way to distribute iPhone applications.

\subsection{The Development Tools}

To develop an application for iPhone, a specific Framework is required.\\

First of all, any software developed for iPhone must be programmed in Objective-C, although it is possible to call C and C++ functions from the code.\\

Objective-C is an Object-Oriented (OO) reflexive programming language build upon the C language. It is comparable to C++ from this point, but differs greatly in many ways, especially by its dynamic message passing system, by being weakly typed and by being able to perform dynamic loading. In its latest version, Objective-C also features a Garbage Collector which abstract the programmer of the memory management consideration. Unfortunately, this feature is not available for iPhone development.\\

In order to compile code for the iPhone, the use of Xcode as Integrated Development Environment is almost unavoidable. Apple has a highly proprietary approach for its products, and programming for an Apple environment is much restrictive to this regard. Fortunately, Xcode and the set of tools provided by Apple offer a great comfort of use in many cases, especially for debugging, or for creating interfaces with the use of an Interface Builder (IB).\\

Once Xcode is set up, the iPhone Software Development Kit must be installed on Xcode. The iPhone SDK takes advantage of all the features of Xcode, including its set of external tool to the largest extent. After being registered to the iPhone developer's program, Xcode can also be linked directly to an actual iPhone device in order to test an application in real-time with any monitoring tool available.\\

This SDK contains Application Programming Interfaces (API) to all the accessible libraries of the iPhone, including Cocoa for the user interface. Unfortunately, many functionalities such as the access to raw data from the video camera are not part of a public API, so their libraries are considered as not accessible and therefore are forbidden by Apple.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Application}
\label{sec:the_application}

\subsection{What it does}

This application has been designed for travellers in Sweden that are looking for a nearby public transport stop. Augmented Reality makes it very intuitive to find a nearby stop: the user just has to "look around" with his iPhone, and the closest stops will appear at their positions. To make it even easier for the user, a Google Map is available when the device is held horizontally if directions are required. \\

This makes the application very interesting for people that are easily confused when reading maps when discovering a new city.\\

But this application has also been designed for people that are already familiar with the city they visit. Simply by pointing at a stop they are interested in, they can get the next departures without having to go till the stop to check the timetable.\\

For now, the application is available for Göteborg and Stockholm, with their respective public transport companies Västtrafik and Storstockholms Lokaltrafik. But additional providers could be added later on.

\subsection{How it works}

The application takes advantage of the GPS capabilities of the iPhone to locate the user on a map. Then thanks to the compass, we are able to estimate the direction he is looking at, and eventually the 3-axis accelerometer allows us to evaluate the angle at which he holds his phone. Note that this application requires a GPS and a compass, and therefore is only compatible with the iPhone 3G-S.\\

Once the position of the user is known, a request is made over the internet to determine the bus stops that are close to him. The answer to this request will be a list of stops with their names and locations together with a forecast list for each of them.\\

An item in the forecast list is composed of a line number, a destination and a set of attributes indicating the time of departure in some information on the quality of the travel.\\

Once the positions of the bus stops are known, we can project them in the virtual space according to the user's coordinates, heading direction and phone holding angle.\\

To cope with the precision errors of acceleration and heading measurement, a buffer is used to get the average values on an arbitrary period of time before updating the knowledge on the user's position. Animations are also used to make transitions between two views appear smoother.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation of the Augmented Reality View}
\label{sec:implementation_of_ar_view}

\subsection{View Hierarchy}

\begin{figure}[ht]
\center
\includegraphics[scale=0.5]{pics/client_view_hierarchy}
\caption{Scheme of the View Hierarchy of the application}
\label{fig:client_view_hierarchy}
\end{figure}

When developing for iPhone, the Model-View-Controller (MVC) is the best architecture to go with. There are well defined classes for views and controllers in the Cocoa framework, so we took advantage of that when designing our application.\\

When using the Cocoa framework, each view must be a subclass of the UIView class, and each controller of the UIViewController class. A view is automatically linked to a view controller.\\

A view can contain other views that will be considered as its subviews, and each view has a frame in which it renders itself. Views can have transparent or semi-transparent backgrounds, so it is possible to create a complex view hierarchy without hassle.\\

Figure \ref{fig:client_view_hierarchy} shows the view hierarchy for our application.\\

In order to render a view on screen, one must define an application window that is a subclass of UIWindow and that will take care of rendering every visible view on the iPhone's screen. By definition, a UIWindow must be the higher node in any view hierarchy.\\

Once the application's window is defined, one should know that there is a serious restriction on the iPhone SDK up to the version 3.2 which prohibits to insert a camera view (UIImagePickerView) as a subview to a UIView. Instead, it must be directly put into a UIWindow. Since we want to use the camera in our application, we have no choice but to put it on top of our view stack. This is semantically very wrong, but there are no alternatives left here by Apple.\\

Furthermore, the camera view itself is locked, so we can't help but use a view called "Camera Overlay" to display any view on top of it. That is ugly and clearly opposed to the concept of our application, but so far there is no other way to do it.\\

As a result, the camera is always rendering in the background of our application, and everything else is rendered on top of it.\\

Now we have a window and a background with a camera view, and we want to be able to switch between a Map View and an Augmented Reality View. To do so, we implement a Main View that will display either of them according to the iPhone position. In order to avoid redundancy, all the data of the application is kept in the Main View Controller and dispatched in its subview, be it a Map View or an Augmented View. The same goes for the acceleration and location data.\\

To present the forecast of a stop, a "modal view" is opened by the Main View Controller.  The display of a modal view will replace the content of the actual window by a temporary view. When dismissing this view, the previous content of the window is loaded again and the application continues.\\

This modal view is a table view itself composed of view cells that are respectively subclasses of UITableView and UITableViewCell. Those view cells are used to display the forecasts list for a bus stop.

\subsection{Positioning within 6 degrees of freedom}

In our application, we need to locate the camera of the iPhone within 6 degrees of freedom:

\begin{itemize}
\item{Latitude}
\item{Longitude}
\item{Altitude}
\item{Azimuth}
\item{Roll angle}
\item{Pitch angle}
\end{itemize}

The GPS directly gives us the Altitude, Latitude and Longitude of the user whereas the compass gives us his Azimuth. This directly takes care of 4 out of the 6 degrees.\\

But thanks to the 3-axis accelerometer, the roll angle and pitch angle can easily be computed by the mean of simple trigonometry as shown in Figure \ref{fig:six_degrees_angles}

\begin{figure}[ht]
\center
\includegraphics[scale=0.5]{pics/six_degrees_angles}
\caption{Projection giving the Roll and Pitch angles, respectively $\theta$ and $\phi$}
\label{fig:six_degrees_angles}
\end{figure}

\subsection{3D Projection}

Now that we know the position of the camera relatively to the object we want to display, we need to project our knowledge on the Augmented View.\\

First, we project a Map on the plane defining the ground. The iPhone has an API to perform 3D projections, by the mean of layer transformations. To apply a transform to a layer, one must provide the transform matrix corresponding to the desired projection.\\

In this case, the transformation will be a rotation of $\phi = \pi/2$ on the X-axis and another $\theta$ corresponding to the Azimuth on the Y-Axis, followed by translations $t_y$ and $t_z$ on the Y and Z axis to give depth to the view, which gives the following matrix:\\

\[ \left( \begin{array}{cccc}
	\textrm{cos} \theta 				& 0 					& -\textrm{sin} \theta 				& - \textrm{sin} \theta / e_z\\
	\textrm{sin} \phi \textrm{ sin} \theta 	& \textrm{cos} \phi		& \textrm{sin} \phi \textrm{ cos} \theta 	& \textrm{sin} \phi \textrm{ cos} \theta  / e_z\\
	\textrm{cos} \phi \textrm{ sin} \theta	& -\textrm{sin} \phi		& \textrm{cos} \phi \textrm{ cos} \theta	& \textrm{cos} \phi \textrm{ cos} \theta / e_z\\
	0							& t_y					& t_z 							& t_z / e_z
\end{array} \right)\]\\

where $e_z$ is the distance of the user from the projection plan. Since we have $\phi = \pi / 2$, the transformation matrix becomes: \\

\[ \left( \begin{array}{cccc}
	\textrm{cos} \theta	& 0 				& -\textrm{sin} \theta 	& - \textrm{sin} \theta / e_z\\
	\textrm{sin} \theta 	& 0				& \textrm{cos} \theta 		& \textrm{cos} \theta  / e_z\\
	0				& -1				& 0					& 0\\
	0				& t_y				& t_z 				& t_z / e_z
\end{array} \right)\]\\

Once the map is projected on the proper plane, we can project the buttons representing the bus stops. First, we need to compute the distance and the azimuth of the stop relatively to the camera.\\

To compute the distance $d$, we use a simplified version of the great-circle formula as shown in Equation \ref{equ:great_circle_projection} where $\phi_s$ and $\lambda_s$ are the Latitude and Longitude of the standpoint (camera position), $\phi_f$ and $\lambda_f$ the ones of the forepoint (bus stop location) and $R$ is the average radius of Earth ($\approx6371$km).

\begin{equation}
\label{equ:great_circle_projection}
d =  R \times \textrm{arccos}\left(\textrm{sin}\phi_s \textrm{ sin}\phi_f + \textrm{cos}\phi_s \textrm{ cos}\phi_f \textrm{ cos}(\lambda_f-\lambda_s)\right)
\end{equation}

To compute the azimuth, we use another formula